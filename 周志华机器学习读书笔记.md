## **第二章** 模型评估与选择

* ​





## 第三章 线性模型

* 广义线性模型：$y = g^{-1}(w^Tx+b)$，其中函数$g$称为联系函数（link function）；

* 对数几率回归：使用Sigmoid函数作为联系函数进行映射，Sigmoid函数为$g = \frac{1}{1+e^{-x}}$；

  对数几率回归函数变换之后变成 $\ln(\frac{y}{1-y})=w^Tx+b$，其中经过变换$p(y=1|x) = \frac{e^{w^Tx+b}}{1+e^{w^Tx+b}}$，$p(y=0|x) = \frac{1}{1+e^{w^Tx+b}}$，使用极大似然方法可以得到相关参数，

* LDA问题：

* 多分类问题:

* 类别不平衡的处理：

  * 再缩放(rescaling)
  * 欠采样：减少样本数较多的类别的样本，以平衡正负样本，典型算法EasyEnsemble算法
  * 过采样：增加样本数较少的类别的样本（并非重复采样），以平衡正负样本，典型算法SMOTE算法
  * 阈值移动：处理最后概率是使用样本比例进行缩放。

## 第四章 决策树







## 第五章 神经网络



## 第六章 支持向量机

## 第七章 贝叶斯分类器

## 第八章 集成学习

* 集成学习：Ensemble Learning，通过结合多个学习器完成学习任务，亦称多分类器系统。

* 按照个体学习器分类：
  * 同质，个体学习器相同，单个学习器称为基学习器；
  * 异质，个体学习器不同，单个学习器称为组件/个体学习器；

* 按照集成学习器的生成方式分类：
  * 串行：个体学习器之间有强依赖关系，必须穿行生成序列化方法，代表
    * Boosting，Adaboost
    * GBDT，Xgboost
  * 并行：个体学习器不存在依赖关系，可并行训练，计算，代表有
    * Bagging
    * 随机森林

* 集成学习器要求：
  * 准确度：个体学习器需要具备一定的准确度；
  * 多样性：学习器之间具备一定的差异性；
  * 两者联系：相矛盾，准确度上升后，要想提升多样性，就会牺牲准确度。

* 分歧、误差分析：

  * 分歧是指基学习器和最后强化学习器之间的不一致性
  * 分歧误差分析公司：$E = \bar{E} -\bar{A}$，其中$E$ 表示集成学习器的平方误差，$\bar{E}$和$\bar{A}$分别表示个体学习器的加权平方误差和以及分歧
  * 缺点：对$\bar{E} -\bar{A}$无法直接优化，$\bar{A}$不是可以直接操作的多样性度量，只能在集成构造完才能进行估计，上述估计只适用于回归，不适用于分类。

* 多样性度量：

  * 典型做法：考虑个体分类器的两两相似和两两不相似

  * 度量标准：

    ![微信图片编辑_20180313143229](C:\Users\rongchangxing\Desktop\微信图片编辑_20180313143229.jpg)

    其中$a+b+c+d=m$

    * 不合度量：$dis_{ij}=\frac{b+c}{m}$

    * 相关系数：$\rho_{ij} = \frac{ad-bc}{\sqrt{(a+b)(a+c)(c+d)(b+d)}}$，$\rho$的值域为$[-1,1]$

    * Q统计量：$Q_{ij} = \frac{ad-bc}{ad+bc}$,并且$|Q_{ij}|<=|\rho_{ij}|$

    * k统计量，定义
      $$
      p1 = \frac{a+d}{m} \\
      p2= \frac{(a+b)(a+c)+(c+d)(b+d)}{m^2} \\
      k = \frac{p_1-p2}{1-p2}
      $$
      其中，$p_1$是两个分类器取得一致的概率，$p_2$是分类器偶然达成一致的概率，如果两个分类器在数据集上完全一致，则$k=1$，如果偶然一致，则$k=0$，$k$一般为非负值，仅在两个分类器达成一致的概率甚至低于偶然性的情况下取负值。

* 多样性增强：

  * 数据样本扰动：对不稳定的基学习器有效（决策树，神经网络），对稳定的基学习器效果不大（线性学习器，支持向量机，朴素贝叶斯，k近邻学习器）
  * 输入属性扰动：
    * 随机子空间算法：从初始属性中抽取出若干的属性子集（subspace），对每一个子集训练基学习器；
    * 随机子空间算法优点：提高多样性，减少属性还可以减少训练开销；
    * 随机子空间算法局限：对于冗余属性少或者包含少量样本的数据集不适用随机子空间算法。
  * 输出表示扰动：对输出表示进行操作以增强多样性。
    * 翻转法：改变一些训练样本的标签；
    * 输出调制法：分类输出转化成回归输出；
    * 将原任务拆分为多个子任务同时执行，例如利用纠错码将多分类任务拆解为一系列二分类任务来训练基学习器；
    * 算法参数扰动：例如改变神经网络的隐层节点数，初始权重，学习率等；
  * 算法参数扰动：改变算法相关参数。




## 第九章 聚类

* 聚类：一种典型的无监督学习方法，通过对无标记训练样本的学习来揭示数据的内在的性质和规律。

* 簇标记向量：$\lambda = (\lambda_1;\lambda_2;...;\lambda_m), \lambda_i \in \{1,2,3,...,k\}$

* 聚类性能度量（有效性指标）：

  * 簇间基本度量指标：

    对数据集$D = \{x_1,x_2,...,x_m \}$，假定通过聚类给出的簇划分和参考模型给出的簇划分分别为 $\mathcal C = \{C_1,C_2,...,C_k \} $和 $\mathcal C^*= \{C_1^*,C_2^*,...,C_k^* \} $,相应的簇标记向量为$\lambda$和$\lambda^*$;
    $$
    a = |SS|,SS=\{ (x_i,x_j) | \lambda_i = \lambda_j , \lambda_i^* = \lambda_j^*,i<j \} \\
    b = |SD|,SD=\{ (x_i,x_j) | \lambda_i = \lambda_j , \lambda_i^* \neq \lambda_j^*,i<j \} \\
    c = |DS|,DS=\{ (x_i,x_j) | \lambda_i \neq \lambda_j , \lambda_i^* = \lambda_j^*,i<j \} \\
    d = |DD|,DD=\{ (x_i,x_j) | \lambda_i \neq \lambda_j , \lambda_i^* \neq \lambda_j^*,i<j \}
    $$
    其中$a,b,c,d$满足$a+b+c+d = m(m-1)/2$

  * 簇间相似度低：以下度量指标均在$[0,1]$

    * Jaccard Coefficient，JC系数：
      $$
      JC= \frac{a}{a+b+c}
      $$

    * FM指数，Fowlkes and Mallows Index，FMI：

    * $$
      FMI = \sqrt{\frac{a}{a+b}  \cdot  \frac{a}{a+c}}
      $$

    * Rand指数，Rand Index, RI：
      $$
      RI= \frac{2(a+d)}{m(m-1)}
      $$

  * 簇间基本度量指标：

    基于聚类的划分$\mathcal C$，定义以下指标
    $$
    avg(C) = \frac{}{}
    $$
    ​

    * $$

      $$

      ​










