## **第二章** 模型评估与选择

* ​





## 第三章 线性模型

* 广义线性模型：$y = g^{-1}(w^Tx+b)$，其中函数$g$称为联系函数（link function）；

* 对数几率回归：使用Sigmoid函数作为联系函数进行映射，Sigmoid函数为$g = \frac{1}{1+e^{-x}}$；

  对数几率回归函数变换之后变成 $\ln(\frac{y}{1-y})=w^Tx+b$，其中经过变换$p(y=1|x) = \frac{e^{w^Tx+b}}{1+e^{w^Tx+b}}$，$p(y=0|x) = \frac{1}{1+e^{w^Tx+b}}$，使用极大似然方法可以得到相关参数，

* LDA问题：

* 多分类问题:

* 类别不平衡的处理：

  * 再缩放(rescaling)
  * 欠采样：减少样本数较多的类别的样本，以平衡正负样本，典型算法EasyEnsemble算法
  * 过采样：增加样本数较少的类别的样本（并非重复采样），以平衡正负样本，典型算法SMOTE算法
  * 阈值移动：处理最后概率是使用样本比例进行缩放。

## 第四章 决策树







## 第五章 神经网络



## 第六章 支持向量机

## 第七章 贝叶斯分类器

* 贝叶斯决策：
  * 条件呢风险

## 第八章 集成学习

* 集成学习：Ensemble Learning，通过结合多个学习器完成学习任务，亦称多分类器系统。

* 按照个体学习器分类：
  * 同质，个体学习器相同，单个学习器称为基学习器；
  * 异质，个体学习器不同，单个学习器称为组件/个体学习器；

* 按照集成学习器的生成方式分类：
  * 串行：个体学习器之间有强依赖关系，必须穿行生成序列化方法，代表
    * Boosting，Adaboost
    * GBDT，Xgboost
  * 并行：个体学习器不存在依赖关系，可并行训练，计算，代表有
    * Bagging
    * 随机森林

* 集成学习器要求：
  * 准确度：个体学习器需要具备一定的准确度；
  * 多样性：学习器之间具备一定的差异性；
  * 两者联系：相矛盾，准确度上升后，要想提升多样性，就会牺牲准确度。

* 分歧、误差分析：

  * 分歧是指基学习器和最后强化学习器之间的不一致性
  * 分歧误差分析公司：$E = \bar{E} -\bar{A}$，其中$E$ 表示集成学习器的平方误差，$\bar{E}$和$\bar{A}$分别表示个体学习器的加权平方误差和以及分歧
  * 缺点：对$\bar{E} -\bar{A}$无法直接优化，$\bar{A}$不是可以直接操作的多样性度量，只能在集成构造完才能进行估计，上述估计只适用于回归，不适用于分类。

* 多样性度量：

  * 典型做法：考虑个体分类器的两两相似和两两不相似

  * 度量标准：

    ![微信图片编辑_20180313143229](C:\Users\rongchangxing\Desktop\微信图片编辑_20180313143229.jpg)

    其中$a+b+c+d=m$

    * 不合度量：$dis_{ij}=\frac{b+c}{m}$

    * 相关系数：$\rho_{ij} = \frac{ad-bc}{\sqrt{(a+b)(a+c)(c+d)(b+d)}}$，$\rho$的值域为$[-1,1]$

    * Q统计量：$Q_{ij} = \frac{ad-bc}{ad+bc}$,并且$|Q_{ij}|<=|\rho_{ij}|$

    * k统计量，定义
      $$
      p1 = \frac{a+d}{m} \\
      p2= \frac{(a+b)(a+c)+(c+d)(b+d)}{m^2} \\
      k = \frac{p_1-p2}{1-p2}
      $$
      其中，$p_1$是两个分类器取得一致的概率，$p_2$是分类器偶然达成一致的概率，如果两个分类器在数据集上完全一致，则$k=1$，如果偶然一致，则$k=0$，$k$一般为非负值，仅在两个分类器达成一致的概率甚至低于偶然性的情况下取负值。

* 多样性增强：

  * 数据样本扰动：对不稳定的基学习器有效（决策树，神经网络），对稳定的基学习器效果不大（线性学习器，支持向量机，朴素贝叶斯，k近邻学习器）
  * 输入属性扰动：
    * 随机子空间算法：从初始属性中抽取出若干的属性子集（subspace），对每一个子集训练基学习器；
    * 随机子空间算法优点：提高多样性，减少属性还可以减少训练开销；
    * 随机子空间算法局限：对于冗余属性少或者包含少量样本的数据集不适用随机子空间算法。
  * 输出表示扰动：对输出表示进行操作以增强多样性。
    * 翻转法：改变一些训练样本的标签；
    * 输出调制法：分类输出转化成回归输出；
    * 将原任务拆分为多个子任务同时执行，例如利用纠错码将多分类任务拆解为一系列二分类任务来训练基学习器；
    * 算法参数扰动：例如改变神经网络的隐层节点数，初始权重，学习率等；
  * 算法参数扰动：改变算法相关参数。




## 第九章 聚类

* 聚类：一种典型的无监督学习方法，通过对无标记训练样本的学习来揭示数据的内在的性质和规律。

* 簇标记向量：$\lambda = (\lambda_1;\lambda_2;...;\lambda_m), \lambda_i \in \{1,2,3,...,k\}$

* 聚类性能度量（有效性指标）：

  * 簇间基本度量指标：

    对数据集$D = \{x_1,x_2,...,x_m \}$，假定通过聚类给出的簇划分和参考模型给出的簇划分分别为 $\mathcal C = \{C_1,C_2,...,C_k \} $和 $\mathcal C^*= \{C_1^*,C_2^*,...,C_k^* \} $,相应的簇标记向量为$\lambda$和$\lambda^*$;
    $$
    a = |SS|,SS=\{ (x_i,x_j) | \lambda_i = \lambda_j , \lambda_i^* = \lambda_j^*,i<j \} \\
    b = |SD|,SD=\{ (x_i,x_j) | \lambda_i = \lambda_j , \lambda_i^* \neq \lambda_j^*,i<j \} \\
    c = |DS|,DS=\{ (x_i,x_j) | \lambda_i \neq \lambda_j , \lambda_i^* = \lambda_j^*,i<j \} \\
    d = |DD|,DD=\{ (x_i,x_j) | \lambda_i \neq \lambda_j , \lambda_i^* \neq \lambda_j^*,i<j \}
    $$
    其中$a,b,c,d$满足$a+b+c+d = m(m-1)/2$

  * 簇间相似度低：以下度量指标均在$[0,1]$

    * Jaccard Coefficient，JC系数：
      $$
      JC= \frac{a}{a+b+c}
      $$

    * FM指数，Fowlkes and Mallows Index，FMI：

    * $$
      FMI = \sqrt{\frac{a}{a+b}  \cdot  \frac{a}{a+c}}
      $$

    * Rand指数，Rand Index, RI：
      $$
      RI= \frac{2(a+d)}{m(m-1)}
      $$

  * 簇间基本度量指标：

    基于聚类的划分$\mathcal C$，定义以下指标

##  第十一章 特征抽取，选择以及稀疏学习 

* 子集搜索和评价


* 子集搜索：

  * 前向搜索
  * 后向搜索
  * 双向搜索

* 子集评价

  * 信息熵
    $$
    Ent(D)= -\sum_{i=1}^{k}p_klog_2p_k
    $$

  * 子集的评价更多的是对原数据集划分的一个度量，任何度量都可以作为该划分的一个评价机制进行特征选择；



* 特征选择方法
  * 过滤式(filter)
  * 包裹式(wrapper)
  * 嵌入式(embedding)



## 第十三章 半监督学习

* 半监督学习：
  * 样本：已知样本有部分没有标签，有部分样本带有标签
  * 处理方式
    * 单独使用有标签的样本，使用监督学习的方式训练模型
    * 单独使用无标签的样本，使用无监督学习方法进行训练
    * 两者相结合，有监督学习加入无标记样本提升效果，无监督加入有标记样本提升效果
* 基本假设：
  * 相似的样本具有相似的输出
  * 聚类假设：假设样本存在簇结构，同一个簇的样本属于同一个类别，只能分类
  * 流形假设：假设数据在一个流形上分布，邻近的样本有相似的输出，输出值没有限制
* 半监督学习的分类：
  * 纯半监督学习：不带标记的样本只是样本，仅仅用作训练
  * 直推学习：不带标记的样本为待预测样本
* 生成式方法
  * 假设：基于生成式模型，假设所有数据都是由同一个潜在的模型生成而来，书中给的是高斯混合模型
  * ​

## 第十四章 概率图模型

* 生成式模型和判别式模型
  * 生成式模型：求联合分布
  * 判别式模型：求条件分布

